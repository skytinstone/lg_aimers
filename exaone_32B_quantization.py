"""
EXAONE 4.0 32B ëª¨ë¸ INT4 ì–‘ìí™”
Meta í…ì„œ ì˜¤ë¥˜ ì™„ì „ í•´ê²° ë²„ì „ (device_map=None ë°©ì‹)
ìˆ˜ì •ì¼: 2026.01.13
ë²„ì „: v3 - device_map ì—†ì´ ë¡œë“œ í›„ ìˆ˜ë™ ë°°ì¹˜
"""

import os
import gc
import time
import shutil

import torch
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class Exaone32BQuantizer:
    def __init__(self):
        self.model_name = "LGAI-EXAONE/EXAONE-4.0-32B"
        self.hf_token = os.getenv("HF_TOKEN")

        if not self.hf_token:
            raise ValueError("HF_TOKENì´ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Device: {self.device}")

        if self.device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"Total VRAM: {total_vram:.2f} GB")

    def quantize_int4_fp16(self, output_dir="./quantized_models/exaone-32b-int4"):
        start_time = time.time()

        print(f"\n{'='*70}")
        print("EXAONE 32B INT4 ì–‘ìí™” (Meta Tensor ì™„ì „ í•´ê²°íŒ v3)")
        print("ì„¤ì •: INT4 Weight (NF4) + FP16 Activation")
        print("ë°©ì‹: device_map=Noneìœ¼ë¡œ ë¡œë“œ í›„ ìˆ˜ë™ ë°°ì¹˜")
        print(f"{'='*70}\n")

        # 4bit ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            llm_int8_enable_fp32_cpu_offload=True,
        )

        offload_dir = "./offload_temp"
        os.makedirs(offload_dir, exist_ok=True)

        model = None
        tokenizer = None

        try:
            print("1. í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token,
                trust_remote_code=True,
            )
            
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("   âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n")

            print("2. 32B ëª¨ë¸ ë¡œë“œ + INT4 ì–‘ìí™” ì¤‘...")
            print("   ì˜ˆìƒ ì‹œê°„: 10-30ë¶„ (ì‹œìŠ¤í…œ ì‚¬ì–‘ì— ë”°ë¼ ë‹¤ë¦„)")
            print("   âš ï¸ ì£¼ì˜: device_map=None ë°©ì‹ìœ¼ë¡œ ë©”íƒ€ í…ì„œ ìƒì„±ì„ ë°©ì§€í•©ë‹ˆë‹¤.")
            print("   ì‹œìŠ¤í…œ RAMì„ ë§ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

            # [í•µì‹¬ ìˆ˜ì •] device_map ì—†ì´ ë¨¼ì € ë¡œë“œí•œ í›„ ìˆ˜ë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ë°°ì¹˜
            # ì´ ë°©ë²•ìœ¼ë¡œ ë©”íƒ€ í…ì„œ ìƒì„±ì„ ì™„ì „íˆ ë°©ì§€í•©ë‹ˆë‹¤.
            print("   [ì§„í–‰ ì¤‘] ëª¨ë¸ ë¡œë“œ ì¤‘ (device_map ì—†ìŒ)...\n")

            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                token=self.hf_token,
                trust_remote_code=True,
                quantization_config=quantization_config,
                device_map=None,  # <--- device_mapì„ Noneìœ¼ë¡œ ì„¤ì •
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                use_safetensors=True,
            )

            print("   [ì§„í–‰ ì¤‘] ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ì¤‘...\n")
            # ì–‘ìí™”ëœ ëª¨ë¸ì„ GPUë¡œ ì´ë™ (VRAMì´ ë¶€ì¡±í•˜ë©´ ìë™ìœ¼ë¡œ CPUë¡œ í´ë°±)
            try:
                model = model.to(self.device)
            except RuntimeError as e:
                print(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {str(e)}")
                print("   CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...\n")
                model = model.to("cpu")
            print("   âœ“ ëª¨ë¸ ë¡œë“œ ë° INT4 ì–‘ìí™” ì™„ë£Œ!\n")

            elapsed_min = (time.time() - start_time) / 60
            print(f"   ë¡œë“œ ê²½ê³¼ ì‹œê°„: {elapsed_min:.1f}ë¶„\n")

            # ëª¨ë¸ì´ ì–´ë–¤ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
            first_param = next(model.parameters())
            model_device = first_param.device
            print(f"3. ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model_device}\n")

            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / (1024**3)
                reserved = torch.cuda.memory_reserved(0) / (1024**3)
                print("4. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
                print(f"   - Allocated: {allocated:.2f} GB")
                print(f"   - Reserved: {reserved:.2f} GB\n")

            # ì¶”ë¡  í…ŒìŠ¤íŠ¸
            print("5. ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
            test_prompt = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜."
            inputs = tokenizer(test_prompt, return_tensors="pt")
            
            # ëª¨ë¸ì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ë””ë°”ì´ìŠ¤ë¡œ ì…ë ¥ ì´ë™
            first_device = next(model.parameters()).device
            inputs = {k: v.to(first_device) for k, v in inputs.items()}

            infer_start = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    temperature=0.5,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                )
            infer_time = time.time() - infer_start

            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"   ì…ë ¥: {test_prompt}")
            print(f"   ì¶œë ¥: {generated}")
            print(f"   ì¶”ë¡  ì‹œê°„: {infer_time:.2f}ì´ˆ")
            print("   âœ“ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")

            # ëª¨ë¸ ì €ì¥
            print(f"6. ëª¨ë¸ ì €ì¥ ì¤‘...")
            print(f"   ê²½ë¡œ: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)

            save_start = time.time()
            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            save_min = (time.time() - save_start) / 60
            
            print(f"   âœ“ ì €ì¥ ì™„ë£Œ (ì†Œìš”: {save_min:.1f}ë¶„)\n")

            self._print_model_info(model)

            total_min = (time.time() - start_time) / 60
            print(f"ì´ ì†Œìš” ì‹œê°„: {total_min:.1f}ë¶„")

            return model, tokenizer

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
            import traceback
            traceback.print_exc()
            raise

        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if model is not None:
                del model
            if tokenizer is not None:
                del tokenizer
            
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            if os.path.exists(offload_dir):
                try:
                    shutil.rmtree(offload_dir)
                    print("\nâœ“ ì„ì‹œ í´ë” ì •ë¦¬ ì™„ë£Œ")
                except:
                    pass

    def _print_model_info(self, model):
        print(f"{'='*70}")
        print("ì–‘ìí™”ëœ 32B ëª¨ë¸ ì •ë³´")
        print(f"{'='*70}")

        total_params = sum(p.numel() for p in model.parameters())
        print(f"ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")

        dtype_counts = {}
        for _, param in model.named_parameters():
            dtype = str(param.dtype)
            dtype_counts[dtype] = dtype_counts.get(dtype, 0) + 1

        print("\në ˆì´ì–´ dtype:")
        for dtype, count in dtype_counts.items():
            print(f"  - {dtype}: {count} layers")

        model_size_gb = total_params * 0.5 / (1024**3)
        print(f"\nì¶”ì • í¬ê¸° (INT4): {model_size_gb:.2f} GB")
        print(f"{'='*70}\n")


def main():
    print("=" * 70)
    print("LG AImers 8ê¸° - EXAONE 32B Quantization Fix")
    print("=" * 70)

    try:
        quantizer = Exaone32BQuantizer()
        quantizer.quantize_int4_fp16()
        
        print("\nğŸ‰ 32B ëª¨ë¸ ì–‘ìí™” ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    main()