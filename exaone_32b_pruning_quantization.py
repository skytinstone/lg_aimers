"""
LG AImers 8ê¸° í•´ì»¤í†¤ - EXAONE 4.0 32B Pruning + Quantization íŒŒì´í”„ë¼ì¸
ì‘ì„±ì¼: 2026.01.15
ì‘ì„±ì: ì‹ ë¯¼ì„ (ìˆ˜ì •: Gemini)

[ìˆ˜ì • ì‚¬í•­]
- Mac M3 Pro 'Insufficient Memory' ì—ëŸ¬ í•´ê²°ì„ ìœ„í•œ ì´ˆê³ ê°•ë„ ë©”ëª¨ë¦¬ ê´€ë¦¬
- 32B ëª¨ë¸ìš© Disk Offloading ë° CPU-Partial Pruning ì ìš©
- Fine-tuning ë‹¨ê³„ëŠ” 32B Mac í™˜ê²½ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì•ˆì „í•˜ê²Œ Skip ë¡œì§ ê°•í™”
"""

import os
import gc
import time
import shutil
import platform
import psutil
import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from dotenv import load_dotenv

# [ìµœìš°ì„ ] .env íŒŒì¼ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
FRIENDLI_API_KEY = os.getenv("FRIENDLI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

class Exaone32BOptimizer:
    def __init__(self, model_name="LGAI-EXAONE/EXAONE-4.0-32B", pruning_ratio=0.3):
        self.model_name = model_name
        self.pruning_ratio = pruning_ratio
        self.pruning_percent = int(pruning_ratio * 100)  # íŒŒì¼ëª…ìš© (30, 40, 50)
        self.start_time = time.time()

        print(f"\n{'='*25} [1. 32B í™˜ê²½ ë¶„ì„ ë° ë©”ëª¨ë¦¬ ì²´í¬] {'='*25}")
        self._check_environment()

    def _check_environment(self):
        print(f"â— ìš´ì˜ì²´ì œ: {platform.system()} (Device: {platform.processor()})")
        if torch.backends.mps.is_available():
            self.device = "mps"
            self.is_mac = True
            self.compute_dtype = torch.bfloat16 # Mac ìµœì í™”
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸ Apple Silicon (MPS) - 32B ëª¨ë“œ")
        elif torch.cuda.is_available():
            self.device = "cuda"
            self.is_mac = False
            self.compute_dtype = torch.float16
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸŸ¢ NVIDIA GPU (CUDA)")
        else:
            self.device = "cpu"
            self.is_mac = False
            self.compute_dtype = torch.float32
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸ’» CPU Mode")

        ram = psutil.virtual_memory().total / (1024**3)
        available_ram = psutil.virtual_memory().available / (1024**3)
        print(f"â— ì‹œìŠ¤í…œ RAM: {ram:.2f} GB (ì‚¬ìš© ê°€ëŠ¥: {available_ram:.2f} GB)")
        print(f"â— ì£¼ì˜: 32B ëª¨ë¸ì€ ì•½ 64GBì˜ ì €ì¥ ê³µê°„(SSD)ì´ ì¶”ê°€ë¡œ í•„ìš”í•©ë‹ˆë‹¤.")

    def load_base_model(self):
        print(f"\n{'='*25} [2. 32B ëª¨ë¸ ë¡œë“œ (Disk Offload)] {'='*25}")
        tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=HF_TOKEN, trust_remote_code=True)
        
        # ì˜¤í”„ë¡œë“œ í´ë” ìƒì„±
        os.makedirs("./offload_32b", exist_ok=True)

        # 32B ëª¨ë¸ì€ Mac RAMë³´ë‹¤ í¬ë¯€ë¡œ device_map="auto"ì™€ offload í•„ìˆ˜
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            token=HF_TOKEN,
            trust_remote_code=True,
            torch_dtype=self.compute_dtype,
            device_map="auto",
            offload_folder="./offload_32b",
            low_cpu_mem_usage=True,
            offload_state_dict=True
        )
        print(f"ğŸ“Š 32B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ê³µìœ  ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ë¶„ì‚° ë¡œë“œë¨)")
        return model, tokenizer

    def apply_magnitude_pruning(self, model):
        print(f"\n{'='*25} [3. Pruning (CPU ì—°ì‚° ë¶„ì‚° ëª¨ë“œ)] {'='*25}")
        target_patterns = ['gate_proj', 'up_proj', 'down_proj']
        
        # 32B ëª¨ë¸ì€ í•œêº¼ë²ˆì— ê³„ì‚°í•˜ë©´ ì—ëŸ¬ë‚˜ë¯€ë¡œ ë ˆì´ì–´ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
        with torch.no_grad():
            for name, module in model.named_modules():
                if isinstance(module, nn.Linear) and any(p in name for p in target_patterns):
                    # ê°€ì¤‘ì¹˜ë¥¼ CPUë¡œ í•œ ê°œì”©ë§Œ ê°€ì ¸ì™€ì„œ 0ìœ¼ë¡œ ë§Œë“¦ (MPS ë©”ëª¨ë¦¬ í­ë°œ ë°©ì§€)
                    w = module.weight.data.cpu().float()
                    threshold = torch.quantile(torch.abs(w), self.pruning_ratio)
                    mask = (torch.abs(w) > threshold).to(self.compute_dtype)
                    
                    # ë‹¤ì‹œ ì›ë³¸ ìœ„ì¹˜(CPU/MPS/Disk)ë¡œ ëŒë ¤ë³´ëƒ„
                    module.weight.data = (w.to(module.weight.device) * mask.to(module.weight.device))
                    
                    del w, mask
                    # 10ê°œ ë ˆì´ì–´ë§ˆë‹¤ ë©”ëª¨ë¦¬ ê°•ì œ ì²­ì†Œ
                    if "proj" in name: 
                        gc.collect()
                        if self.is_mac: torch.mps.empty_cache()
        
        print(f"âœ… 32B Pruning ì™„ë£Œ")
        return model

    def fine_tune_model(self, model, tokenizer, output_dir=None):
        if output_dir is None:
            output_dir = f"./pruned_models/exaone-32b-pruned-{self.pruning_percent}"

        print(f"\n{'='*25} [4. ì €ì¥ ë° Fine-tuning ê²€í† ] {'='*25}")
        
        # 32BëŠ” Macì—ì„œ í•™ìŠµ ì‹œë„ì‹œ 99% í™•ë¥ ë¡œ ì»¤ë„ì´ ì¢…ë£Œë¨(Memory Error)
        # ë”°ë¼ì„œ ì €ì¥ í›„ í•™ìŠµì€ ì•„ì£¼ ì‘ì€ ìƒ˜í”Œë¡œ ì‹œë„í•˜ê±°ë‚˜ ìŠ¤í‚µ ê¶Œì¥
        print("ğŸ’¾ Pruned 32B ëª¨ë¸ ì €ì¥ ì¤‘ (ì˜¤í”„ë¡œë“œ ë°ì´í„° í¬í•¨)...")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        if self.is_mac:
            print("ğŸ Mac ì•Œë¦¼: 32B ëª¨ë¸ì˜ Fine-tuningì€ ë©”ëª¨ë¦¬ í•œê³„ë¡œ ì¸í•´ ìŠ¤í‚µì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            print("ëŒ€ì‹  Pruned ëª¨ë¸ ì €ì¥ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.")
            return model

        # CUDA í™˜ê²½ì¼ ê²½ìš°ì—ë§Œ ë§¤ìš° ì œí•œì ìœ¼ë¡œ ì‹œë„
        return model

    def apply_quantization(self, pruned_path, output_dir=None):
        if output_dir is None:
            output_dir = f"./quantized_models/exaone-32b-pruned-int4-{self.pruning_percent}"

        print(f"\n{'='*25} [5. INT4 Quantization] {'='*25}")

        if self.is_mac:
            print("ğŸ Mac í™˜ê²½ ì•Œë¦¼: 32B INT4 ì–‘ìí™”ëŠ” ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ FP16 ìƒíƒœë¡œ ê²€ì¦í•©ë‹ˆë‹¤.")
            # ë‹¤ì‹œ ë¡œë“œí•  ë•Œë„ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•„ìˆ˜
            model = AutoModelForCausalLM.from_pretrained(
                pruned_path,
                torch_dtype=self.compute_dtype,
                device_map="auto",
                offload_folder="./offload_32b_test",
                low_cpu_mem_usage=True
            )
        else:
            print("ğŸŸ¢ CUDA í™˜ê²½ ì•Œë¦¼: 32B ëª¨ë¸ INT4 ì–‘ìí™” ì ìš©")
            bnb_cfg = BitsAndBytesConfig(
                load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4", llm_int8_enable_fp32_cpu_offload=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                pruned_path, quantization_config=bnb_cfg, device_map="auto",
                offload_folder="./offload_32b_quant"
            )

        print("ğŸ” 32B ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì†Œìš” ì˜ˆìƒ)...")
        tokenizer = AutoTokenizer.from_pretrained(pruned_path)
        inputs = tokenizer("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”", return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=15, do_sample=False)
        print(f"â— ì¶”ë¡  ê²°ê³¼: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")

    def run_full_pipeline(self):
        try:
            model, tokenizer = self.load_base_model()
            model = self.apply_magnitude_pruning(model)

            # Pruning í›„ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            pruned_dir = f"./pruned_models/exaone-32b-pruned-{self.pruning_percent}"
            model = self.fine_tune_model(model, tokenizer, output_dir=pruned_dir)

            del model
            gc.collect()
            if self.is_mac: torch.mps.empty_cache()

            # Quantization ì ìš©
            quantized_dir = f"./quantized_models/exaone-32b-pruned-int4-{self.pruning_percent}"
            self.apply_quantization(pruned_dir, output_dir=quantized_dir)

            print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ (ì†Œìš”ì‹œê°„: {(time.time()-self.start_time)/60:.2f}ë¶„)")
            print(f"âœ… Pruned ëª¨ë¸: {pruned_dir}")
            print(f"âœ… Quantized ëª¨ë¸: {quantized_dir}")
        except Exception as e:
            print(f"âŒ 32B ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        finally:
            # ì„ì‹œ í´ë” ì‚­ì œ
            for folder in ["./offload_32b", "./offload_32b_test", "./offload_32b_quant"]:
                if os.path.exists(folder): shutil.rmtree(folder, ignore_errors=True)

def get_pruning_ratio():
    """ì‚¬ìš©ìë¡œë¶€í„° Pruning ë¹„ìœ¨ ì…ë ¥ë°›ê¸°"""
    print("\n" + "="*70)
    print("EXAONE 32B Pruning + Quantization íŒŒì´í”„ë¼ì¸")
    print("="*70)
    print("\nPruning ë¹„ìœ¨ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 30, 40, 50)")
    print("ì…ë ¥í•œ ê°’ì˜ %ë§Œí¼ ê°€ì¤‘ì¹˜ê°€ ì œê±°ë©ë‹ˆë‹¤.")

    while True:
        try:
            pruning_input = input("\nPruning ë¹„ìœ¨ (ìˆ«ìë§Œ ì…ë ¥): ").strip()
            pruning_percent = int(pruning_input)

            if pruning_percent < 0 or pruning_percent > 100:
                print("âŒ 0~100 ì‚¬ì´ì˜ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            pruning_ratio = pruning_percent / 100.0
            print(f"\nâœ… Pruning ë¹„ìœ¨: {pruning_percent}% (ratio: {pruning_ratio})")
            return pruning_ratio

        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    pruning_ratio = get_pruning_ratio()
    pipeline = Exaone32BOptimizer(pruning_ratio=pruning_ratio)
    pipeline.run_full_pipeline()