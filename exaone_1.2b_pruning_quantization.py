"""
LG AImers 8ê¸° í•´ì»¤í†¤ - EXAONE 4.0 1.2B Pruning + Quantization íŒŒì´í”„ë¼ì¸
ì‘ì„±ì¼: 2026.01.15
ì‘ì„±ì: ì‹ ë¯¼ì„ (ìˆ˜ì •: Gemini)

[ìˆ˜ì • ì‚¬í•­] 
- Mac 'Insufficient Memory' ì—ëŸ¬ í•´ê²°ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©
- bfloat16 ì‚¬ìš© ë° ë°ì´í„°ì…‹ ìƒ˜í”Œë§ ì¶•ì†Œ
"""

import os
import gc
import time
import platform
import psutil
import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from dotenv import load_dotenv

# [ìµœìš°ì„ ] .env íŒŒì¼ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
FRIENDLI_API_KEY = os.getenv("FRIENDLI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

class Exaone12BOptimizer:
    def __init__(self, model_name="LGAI-EXAONE/EXAONE-4.0-1.2B", pruning_ratio=0.3):
        self.model_name = model_name
        self.pruning_ratio = pruning_ratio
        self.pruning_percent = int(pruning_ratio * 100)  # íŒŒì¼ëª…ìš© (30, 40, 50)
        self.start_time = time.time()

        print(f"\n{'='*25} [1. í™˜ê²½ ë¶„ì„ ë° ë©”ëª¨ë¦¬ ì²´í¬] {'='*25}")
        self._check_environment()

    def _check_environment(self):
        print(f"â— ìš´ì˜ì²´ì œ: {platform.system()} (Device: {platform.processor()})")
        if torch.backends.mps.is_available():
            self.device = "mps"
            self.is_mac = True
            # Mac ì „ìš©: bfloat16ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
            self.compute_dtype = torch.bfloat16 
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸ Apple Silicon (MPS) - bfloat16 ëª¨ë“œ ì‚¬ìš©")
        elif torch.cuda.is_available():
            self.device = "cuda"
            self.is_mac = False
            self.compute_dtype = torch.float16
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸŸ¢ NVIDIA GPU (CUDA)")
        else:
            self.device = "cpu"
            self.is_mac = False
            self.compute_dtype = torch.float32
            print(f"â— ì—°ì‚° ì¥ì¹˜: ğŸ’» CPU Mode")

        ram = psutil.virtual_memory().total / (1024**3)
        available_ram = psutil.virtual_memory().available / (1024**3)
        print(f"â— ì‹œìŠ¤í…œ RAM: {ram:.2f} GB (í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥: {available_ram:.2f} GB)")

    def _get_model_size(self, model):
        try:
            param_size = sum(p.nelement() * p.element_size() for p in model.parameters())
            return param_size / (1024**2)
        except: return 0.0

    def load_base_model(self):
        print(f"\n{'='*25} [2. ëª¨ë¸ ë¡œë“œ] {'='*25}")
        tokenizer = AutoTokenizer.from_pretrained(self.model_name, token=HF_TOKEN, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # low_cpu_mem_usage=True ì˜µì…˜ ì¶”ê°€ë¡œ ë¡œë“œ ì‹œ ë©”ëª¨ë¦¬ í”¼í¬ ë°©ì§€
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            token=HF_TOKEN,
            trust_remote_code=True,
            torch_dtype=self.compute_dtype,
            device_map=self.device,
            low_cpu_mem_usage=True 
        )
        print(f"ğŸ“Š ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬: {self._get_model_size(model):.2f} MB)")
        return model, tokenizer

    def apply_magnitude_pruning(self, model):
        print(f"\n{'='*25} [3. Pruning] {'='*25}")
        target_patterns = ['gate_proj', 'up_proj', 'down_proj']
        
        with torch.no_grad():
            for name, module in model.named_modules():
                if isinstance(module, nn.Linear) and any(p in name for p in target_patterns):
                    # MPS ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ê³„ì‚°í•  ë•Œë§Œ CPUë¡œ ë³µì‚¬
                    w = module.weight.data.cpu().float()
                    threshold = torch.quantile(torch.abs(w), self.pruning_ratio)
                    mask = (torch.abs(w) > threshold).to(self.compute_dtype)
                    module.weight.data = (w.to(self.device) * mask.to(self.device))
                    del w, mask
        
        # ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if self.is_mac: torch.mps.empty_cache()
        print(f"âœ… Pruning ì™„ë£Œ")
        return model

    def fine_tune_model(self, model, tokenizer, output_dir=None):
        if output_dir is None:
            output_dir = f"./pruned_models/exaone-1.2b-pruned-{self.pruning_percent}"

        print(f"\n{'='*25} [4. Fine-tuning (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)] {'='*25}")
        
        try:
            # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë” ì¤„ì„ (100 -> 20)
            dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:20]")
            def tokenize_fn(e): return tokenizer(e["text"], truncation=True, max_length=64, padding="max_length")
            tokenized_ds = dataset.map(tokenize_fn, batched=True).filter(lambda x: len(x['input_ids']) > 0)

            model.config.use_cache = False
            # Mac MPSì—ì„œ ë©”ëª¨ë¦¬ í­ë°œì„ ë§‰ê¸° ìœ„í•œ ì´ˆê²½ëŸ‰ ì„¤ì •
            args = TrainingArguments(
                output_dir=output_dir,
                per_device_train_batch_size=1, # ìµœì†Œí™”
                gradient_accumulation_steps=4,
                num_train_epochs=1,
                learning_rate=1e-5,
                # Macì—ì„œëŠ” fp16=Trueë³´ë‹¤ bf16=False(ë˜ëŠ” ê¸°ë³¸)ê°€ ë” ì•ˆì „í•  ìˆ˜ ìˆìŒ
                fp16=False, 
                bf16=False,
                save_strategy="no",
                report_to="none",
                logging_steps=1
            )
            trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds, 
                              data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))
            
            print("í•™ìŠµ ì‹œì‘...")
            trainer.train()
            print("âœ… Fine-tuning ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Fine-tuning ê±´ë„ˆëœ€ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜ˆìƒ): {e}")

        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        return model

    def apply_quantization(self, pruned_path, output_dir=None):
        if output_dir is None:
            output_dir = f"./quantized_models/exaone-1.2b-pruned-int4-{self.pruning_percent}"

        print(f"\n{'='*25} [5. INT4 Quantization] {'='*25}")

        # Quantization ë‹¨ê³„ì—ì„œ ë‹¤ì‹œ ë¡œë“œí•  ë•Œ ë©”ëª¨ë¦¬ í™•ë³´
        model = AutoModelForCausalLM.from_pretrained(
            pruned_path,
            torch_dtype=self.compute_dtype,
            device_map=self.device,
            low_cpu_mem_usage=True
        )

        tokenizer = AutoTokenizer.from_pretrained(pruned_path)

        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        inputs = tokenizer("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”", return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False)
        print(f"â— ì¶”ë¡  ê²°ê³¼: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")

    def run_full_pipeline(self):
        try:
            model, tokenizer = self.load_base_model()
            model = self.apply_magnitude_pruning(model)

            # Pruning í›„ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            pruned_dir = f"./pruned_models/exaone-1.2b-pruned-{self.pruning_percent}"
            model = self.fine_tune_model(model, tokenizer, output_dir=pruned_dir)

            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ í•´ì œ í›„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            del model
            gc.collect()
            if self.is_mac: torch.mps.empty_cache()

            # Quantization ì ìš©
            quantized_dir = f"./quantized_models/exaone-1.2b-pruned-int4-{self.pruning_percent}"
            self.apply_quantization(pruned_dir, output_dir=quantized_dir)

            print(f"\nâœ… ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {(time.time()-self.start_time)/60:.2f}ë¶„)")
            print(f"âœ… Pruned ëª¨ë¸: {pruned_dir}")
            print(f"âœ… Quantized ëª¨ë¸: {quantized_dir}")
        except Exception as e:
            print(f"âŒ ìµœì¢… ì—ëŸ¬: {e}")

def get_pruning_ratio():
    """ì‚¬ìš©ìë¡œë¶€í„° Pruning ë¹„ìœ¨ ì…ë ¥ë°›ê¸°"""
    print("\n" + "="*70)
    print("EXAONE 1.2B Pruning + Quantization íŒŒì´í”„ë¼ì¸")
    print("="*70)
    print("\nPruning ë¹„ìœ¨ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 30, 40, 50)")
    print("ì…ë ¥í•œ ê°’ì˜ %ë§Œí¼ ê°€ì¤‘ì¹˜ê°€ ì œê±°ë©ë‹ˆë‹¤.")

    while True:
        try:
            pruning_input = input("\nPruning ë¹„ìœ¨ (ìˆ«ìë§Œ ì…ë ¥): ").strip()
            pruning_percent = int(pruning_input)

            if pruning_percent < 0 or pruning_percent > 100:
                print("âŒ 0~100 ì‚¬ì´ì˜ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            pruning_ratio = pruning_percent / 100.0
            print(f"\nâœ… Pruning ë¹„ìœ¨: {pruning_percent}% (ratio: {pruning_ratio})")
            return pruning_ratio

        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    pruning_ratio = get_pruning_ratio()
    pipeline = Exaone12BOptimizer(pruning_ratio=pruning_ratio)
    pipeline.run_full_pipeline()